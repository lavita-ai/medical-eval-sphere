{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..') + '/src')\n",
    "\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "\n",
    "from annotation_tools import LabelAnalysis\n",
    "\n",
    "label_analysis = LabelAnalysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first run the script for generating LLM judgments.\n",
    "\n",
    "**`Round 1`**\n",
    "```\n",
    "llm_judge.sh -m openai_gpt-4o-2024-05-13,anthropic_claude-3-5-sonnet-20240620 -d ../data/llm_judge -t ../data/prompts/llm_pairwise_judge_v1.txt\n",
    "\n",
    "```\n",
    "\n",
    "**`Round 2`**\n",
    "```\n",
    "llm_judge.sh -m openai_gpt-4o-2024-08-06,anthropic_claude-3-5-sonnet-20241022 -d ../data/llm_judge -t ../data/prompts/llm_pairwise_judge_v1.txt\n",
    "\n",
    "```\n",
    "\n",
    "To run this script, we first need to put the CSV file of the batch we want to run this script on in the `../data/llm_judge` folder. This script will get that file as input and run the `llm_annotation.py` on it six times. The first three times are based on the default position of `response_a` and `response_b`, and the next three times are based on the reverse order of the responses. The results of these runs will be saved in a new CSV file under the column name `pred_[model_name]_ab|ba[run_number]` where `run_number` is one of `[1, 2, 3]`). The final CSV file for each batch that includes all six columns will be the input to the analysis in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vote inconsistency in [ab] runs by models\r\n",
      "-----------------------------------------\r\n",
      "{'anthropic_claude-3-5-sonnet-20240620': 48, 'openai_gpt-4o-2024-05-13': 174}\r\n",
      "total: 222\r\n",
      "Vote inconsistency in [ba] runs by models\r\n",
      "-----------------------------------------\r\n",
      "{'anthropic_claude-3-5-sonnet-20240620': 69, 'openai_gpt-4o-2024-05-13': 199}\r\n",
      "total: 268\r\n",
      "\r\n",
      "Disagreements between [ab] and [ba] runs by model\r\n",
      "-------------------------------------------------\r\n",
      "* model: anthropic_claude-3-5-sonnet-20240620\r\n",
      "OrderedDict([('efficiency', {'count': 197}),\r\n",
      "             ('bias', {'count': 132}),\r\n",
      "             ('harmfulness', {'count': 95}),\r\n",
      "             ('reasoning', {'count': 74}),\r\n",
      "             ('correctness', {'count': 73}),\r\n",
      "             ('helpfulness', {'count': 52})])\r\n",
      "total: 623\r\n",
      "* model: openai_gpt-4o-2024-05-13\r\n",
      "OrderedDict([('efficiency', {'count': 94}),\r\n",
      "             ('correctness', {'count': 81}),\r\n",
      "             ('reasoning', {'count': 79}),\r\n",
      "             ('helpfulness', {'count': 72}),\r\n",
      "             ('harmfulness', {'count': 71})])\r\n",
      "total: 397\r\n",
      "\r\n",
      "Agreement between the two llm judges\r\n",
      "------------------------------------\r\n",
      "{'chance': 0.2984717013888889,\r\n",
      " 'cohen': np.float64(0.44763454194624186),\r\n",
      " 'pearson': np.float64(0.6168309368196159),\r\n",
      " 'percentage': np.float64(0.6125)}\r\n"
     ]
    }
   ],
   "source": [
    "!python ../src/llm_judge.py --llms openai_gpt-4o-2024-05-13 anthropic_claude-3-5-sonnet-20240620"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse vote for anthropic_claude-3-5-sonnet-20241022: expected string or bytes-like object. Using neutral vote.\n",
      "Vote inconsistency in [ab] runs by models\n",
      "-----------------------------------------\n",
      "{'anthropic_claude-3-5-sonnet-20241022': 53, 'openai_gpt-4o-2024-08-06': 138}\n",
      "total: 191\n",
      "Vote inconsistency in [ba] runs by models\n",
      "-----------------------------------------\n",
      "{'anthropic_claude-3-5-sonnet-20241022': 69, 'openai_gpt-4o-2024-08-06': 144}\n",
      "total: 213\n",
      "\n",
      "Disagreements between [ab] and [ba] runs by model\n",
      "-------------------------------------------------\n",
      "* model: anthropic_claude-3-5-sonnet-20241022\n",
      "OrderedDict([('efficiency', {'count': 181}),\n",
      "             ('correctness', {'count': 76}),\n",
      "             ('helpfulness', {'count': 63}),\n",
      "             ('reasoning', {'count': 52}),\n",
      "             ('harmfulness', {'count': 38}),\n",
      "             ('bias', {'count': 7})])\n",
      "total: 417\n",
      "* model: openai_gpt-4o-2024-08-06\n",
      "OrderedDict([('efficiency', {'count': 103}),\n",
      "             ('reasoning', {'count': 88}),\n",
      "             ('helpfulness', {'count': 77}),\n",
      "             ('correctness', {'count': 72}),\n",
      "             ('harmfulness', {'count': 54})])\n",
      "total: 394\n",
      "\n",
      "Agreement between the two llm judges\n",
      "------------------------------------\n",
      "{'chance': 0.36850503472222224,\n",
      " 'cohen': np.float64(0.5467105586913329),\n",
      " 'pearson': np.float64(0.5345855610650536),\n",
      " 'percentage': np.float64(0.71375)}\n"
     ]
    }
   ],
   "source": [
    "!python ../src/llm_judge.py --llms openai_gpt-4o-2024-08-06 anthropic_claude-3-5-sonnet-20241022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval-sphere",
   "language": "python",
   "name": "eval-sphere"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
