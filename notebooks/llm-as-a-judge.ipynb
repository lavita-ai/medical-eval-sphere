{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..') + '/src')\n",
    "\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "\n",
    "from annotation_tools import LabelAnalysis\n",
    "\n",
    "label_analysis = LabelAnalysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first run the script for generating LLM judgments.\n",
    "\n",
    "**`Round 1`**\n",
    "```\n",
    "llm_judge.sh -m openai_gpt-4o-2024-05-13,anthropic_claude-3-5-sonnet-20240620 -d ../data/llm_judge -t ../data/prompts/llm_pairwise_judge_v1.txt\n",
    "\n",
    "```\n",
    "\n",
    "**`Round 2`**\n",
    "```\n",
    "llm_judge.sh -m openai_gpt-4o-2024-08-06,anthropic_claude-3-5-sonnet-20241022 -d ../data/llm_judge -t ../data/prompts/llm_pairwise_judge_v1.txt\n",
    "\n",
    "```\n",
    "\n",
    "To run this script, first, place the CSV file of the batch you want to process in the `../data/llm_judge` folder. Ensure there is only one CSV file in the folder (`TODO`: remove this requirement). The script will use that file as input and run `llm_annotation.py` on it six times for each model. The first three runs will use the default order of `response_a` and `response_b`, and the next three will use the reversed order of the responses.\n",
    "\n",
    "The results of these runs will be saved in multiple CSV files (one file after each run for checkpoiting), with columns named `pred_[model_name]_ab|ba[run_number]`, where `run_number` is one of `[1, 2, 3]`. For example, `pred_openai_gpt-4o-2024-08-06_ab1` indicates the first of three runs using `openai_gpt-4o-2024-08-06` as the LLM judge, with responses in their default order.\n",
    "\n",
    "The final CSV file for each batch, which includes all six columns per model, will serve as the input for the analysis in the next step. You can repeat this process for a new round, but make sure to remove or move the CSV files generated in the current round beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the `llm_judge.sh` script, navigate to the `../data/llm_judge` folder and ensure that all annotated batch files follow this naming pattern: `batch[batch_number]-llm.csv`. For example: `../data/llm_judge/batch1-llm.csv`. Once all batch file names have been updated to match this pattern, run the following script. (`TODO`: implement auto-renaming as part of running `llm_judge.sh`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vote inconsistency in [ab] runs by models\r\n",
      "-----------------------------------------\r\n",
      "{'anthropic_claude-3-5-sonnet-20240620': 48, 'openai_gpt-4o-2024-05-13': 174}\r\n",
      "total: 222\r\n",
      "Vote inconsistency in [ba] runs by models\r\n",
      "-----------------------------------------\r\n",
      "{'anthropic_claude-3-5-sonnet-20240620': 69, 'openai_gpt-4o-2024-05-13': 199}\r\n",
      "total: 268\r\n",
      "\r\n",
      "Disagreements between [ab] and [ba] runs by model\r\n",
      "-------------------------------------------------\r\n",
      "* model: anthropic_claude-3-5-sonnet-20240620\r\n",
      "OrderedDict([('efficiency', {'count': 197}),\r\n",
      "             ('bias', {'count': 132}),\r\n",
      "             ('harmfulness', {'count': 95}),\r\n",
      "             ('reasoning', {'count': 74}),\r\n",
      "             ('correctness', {'count': 73}),\r\n",
      "             ('helpfulness', {'count': 52})])\r\n",
      "total: 623\r\n",
      "* model: openai_gpt-4o-2024-05-13\r\n",
      "OrderedDict([('efficiency', {'count': 94}),\r\n",
      "             ('correctness', {'count': 81}),\r\n",
      "             ('reasoning', {'count': 79}),\r\n",
      "             ('helpfulness', {'count': 72}),\r\n",
      "             ('harmfulness', {'count': 71})])\r\n",
      "total: 397\r\n",
      "\r\n",
      "Agreement between the two llm judges\r\n",
      "------------------------------------\r\n",
      "{'chance': 0.2984717013888889,\r\n",
      " 'cohen': np.float64(0.44763454194624186),\r\n",
      " 'pearson': np.float64(0.6168309368196159),\r\n",
      " 'percentage': np.float64(0.6125)}\r\n"
     ]
    }
   ],
   "source": [
    "!python ../src/llm_judge.py --llms openai_gpt-4o-2024-05-13 anthropic_claude-3-5-sonnet-20240620"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse vote for anthropic_claude-3-5-sonnet-20241022: expected string or bytes-like object. Using neutral vote.\n",
      "Vote inconsistency in [ab] runs by models\n",
      "-----------------------------------------\n",
      "{'anthropic_claude-3-5-sonnet-20241022': 53, 'openai_gpt-4o-2024-08-06': 138}\n",
      "total: 191\n",
      "Vote inconsistency in [ba] runs by models\n",
      "-----------------------------------------\n",
      "{'anthropic_claude-3-5-sonnet-20241022': 69, 'openai_gpt-4o-2024-08-06': 144}\n",
      "total: 213\n",
      "\n",
      "Disagreements between [ab] and [ba] runs by model\n",
      "-------------------------------------------------\n",
      "* model: anthropic_claude-3-5-sonnet-20241022\n",
      "OrderedDict([('efficiency', {'count': 181}),\n",
      "             ('correctness', {'count': 76}),\n",
      "             ('helpfulness', {'count': 63}),\n",
      "             ('reasoning', {'count': 52}),\n",
      "             ('harmfulness', {'count': 38}),\n",
      "             ('bias', {'count': 7})])\n",
      "total: 417\n",
      "* model: openai_gpt-4o-2024-08-06\n",
      "OrderedDict([('efficiency', {'count': 103}),\n",
      "             ('reasoning', {'count': 88}),\n",
      "             ('helpfulness', {'count': 77}),\n",
      "             ('correctness', {'count': 72}),\n",
      "             ('harmfulness', {'count': 54})])\n",
      "total: 394\n",
      "\n",
      "Agreement between the two llm judges\n",
      "------------------------------------\n",
      "{'chance': 0.36850503472222224,\n",
      " 'cohen': np.float64(0.5467105586913329),\n",
      " 'pearson': np.float64(0.5345855610650536),\n",
      " 'percentage': np.float64(0.71375)}\n"
     ]
    }
   ],
   "source": [
    "!python ../src/llm_judge.py --llms openai_gpt-4o-2024-08-06 anthropic_claude-3-5-sonnet-20241022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval-sphere",
   "language": "python",
   "name": "eval-sphere"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
